{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "164e417a-6170-4256-913f-7944d0c3cdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 855 chunks from 83 documents.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import google.generativeai as genai\n",
    "\n",
    "load_dotenv(\"/home/harish/Desktop/python/chatbot/.env\")\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=api_key) \n",
    "model_name = \"gemini-1.5-flash\"\n",
    "\n",
    "def chunk_text(text, max_length=500):\n",
    "    \"\"\"Splits a single text string into chunks of a max length.\"\"\"\n",
    "    return [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "# 1. FIND ALL FILE PATHS (Your code is good)\n",
    "doc_path = \"/home/harish/Desktop/python/chatbot/docs/**/*\" # Using '/*' is slightly more robust\n",
    "doc_files = glob.glob(doc_path, recursive=True)\n",
    "\n",
    "# Filter for specific file extensions\n",
    "doc_files = [f for f in doc_files if f.endswith((\".md\", \".txt\", \".html\"))]\n",
    "\n",
    "if not doc_files:\n",
    "    raise ValueError(\"❌ No documents found. Check your docs path and file extensions!\")\n",
    "\n",
    "chunks = []\n",
    "for f in doc_files:\n",
    "    with open(f, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    for chunk in chunk_text(text):\n",
    "        embedding = genai.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            content=chunk\n",
    "        )\n",
    "        chunks.append({\"text\": chunk, \"vector\": embedding[\"embedding\"]})\n",
    "\n",
    "if not chunks:\n",
    "    raise ValueError(\"❌ No chunks created. Are your documents empty?\")\n",
    "\n",
    "print(f\"✅ Loaded {len(chunks)} chunks from {len(doc_files)} documents.\")\n",
    "\n",
    "# Cosine similarity helper\n",
    "def cosine_similarity(v1, v2):\n",
    "    dot = sum(a * b for a, b in zip(v1, v2))\n",
    "    mag1 = math.sqrt(sum(x ** 2 for x in v1))\n",
    "    mag2 = math.sqrt(sum(x ** 2 for x in v2))\n",
    "    return dot / (mag1 * mag2)\n",
    "\n",
    "# Setup Chat Model\n",
    "chat_model = genai.GenerativeModel(model_name)\n",
    "\n",
    "# Ask Bot function\n",
    "def ask_bot(question: str) -> str:\n",
    "    \"\"\"Ask the chatbot a question based on your docs.\"\"\"\n",
    "    # Embed question\n",
    "    q_embed = genai.embed_content(\n",
    "        model=\"models/text-embedding-004\",\n",
    "        content=question\n",
    "    )[\"embedding\"]\n",
    "\n",
    "    # Rank chunks by similarity\n",
    "    ranked = sorted(\n",
    "        [(c, cosine_similarity(q_embed, c[\"vector\"])) for c in chunks],\n",
    "        key=lambda x: -x[1]\n",
    "    )[:3]\n",
    "\n",
    "    # Build prompt\n",
    "    context = \"\\n---\\n\".join(c[\"text\"] for c, _ in ranked)\n",
    "    prompt = f\"\"\"\n",
    "    You are an assistant that answers questions based only on the following document excerpts.\n",
    "    If the answer isn't clearly in the documents, reply with \"I don't know.\"\n",
    "    DOCUMENTS:\n",
    "    {context}\n",
    "    QUESTION: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # Ask Gemini\n",
    "    response = chat_model.generate_content([{\"role\": \"user\", \"parts\": [prompt]}])\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2183ed0c-1051-4ccf-9631-c51b737899dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The AWS CLI (Command Line Interface) needs to be installed on your local machine or deployment server.  Installation instructions are provided for macOS (using `brew install awscli`), Ubuntu/Debian (using `sudo apt install awscli`), and a cross-platform method using pip (`pip install awscli`).\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_bot(\"explain about aws cli\") # this data is given inside my docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bb14e69-92f8-4c82-a30e-1b614f758b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_bot(\"kya aapke ghar me pintu hai\")  # this data is not given inside my docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9a30d8c-2130-4624-a8f5-2ee49a0883fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To create an Auto Scaling Group, go to the EC2 Dashboard → Auto Scaling Groups → Create Auto Scaling Group.  Attach the launch template and set the desired, minimum, and maximum instance counts.\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_bot(\"how to create auto scalling group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33885b44-e167-40e1-9f1d-01173562c8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, the document describes a worker pattern using Sidekiq workers for background job processing in the MGR API.  These workers handle time-consuming operations (data import, email/SMS notifications, file processing, external API integrations, and batch operations) to prevent blocking the main application thread.  The architecture uses Sidekiq integration, and includes structured logging for tracking events.  Testing guidance includes using mocks, real data (via fabricators), authorization checks, and performance testing with realistic data volumes.\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_bot(\"do you know worker pettern if yes then explain me about it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095e803b-8013-4718-8415-0a2e8b27b81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
